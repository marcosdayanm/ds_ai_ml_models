{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658916be-475c-47aa-a97a-5c4e1d71514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create training samples\n",
      "Save training samples into a file\n",
      "Create test samples\n",
      "Save test samples into a file\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "\"\"\"\n",
    "Create a random dataset with 100 training samples and 30 test samples\n",
    "The dataset is composed of five features and a classification label (1 or 0 - binary classification)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Create training samples\")\n",
    "feature1=list(np.random.normal(0, 0.2, 100))\n",
    "feature2=list(np.random.normal(0, 0.2, 100))\n",
    "feature3=list(np.random.normal(0, 0.2, 100))\n",
    "feature4=list(np.random.normal(0, 0.2, 100))\n",
    "feature5=list(np.random.normal(0, 0.2, 100))\n",
    "label=list(np.random.choice([1.0, 0.0], size=100, p=[0.5, 0.5]))\n",
    "\n",
    "print(\"Save training samples into a file\")\n",
    "\n",
    "with codecs.open(\"regressionTraining.txt\",\"w\",\"UTF-8\") as file:\n",
    "    for f1,f2,f3,f4,f5,l in zip(feature1,feature2,feature3,feature4,feature5,label):\n",
    "        file.write(str(f1)+\",\"+str(f2)+\",\"+str(f3)+\",\"+str(f4)+\",\"+str(f5)+\",\"+str(l)+\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Create test samples\")\n",
    "feature1=list(np.random.normal(0, 0.2, 50))\n",
    "feature2=list(np.random.normal(0, 0.2, 50))\n",
    "feature3=list(np.random.normal(0, 0.2, 50))\n",
    "feature4=list(np.random.normal(0, 0.2, 50))\n",
    "feature5=list(np.random.normal(0, 0.2, 50))\n",
    "label=list(np.random.choice([1.0, 0.0], size=50, p=[0.5, 0.5]))\n",
    "\n",
    "\n",
    "print(\"Save test samples into a file\")\n",
    "with codecs.open(\"regressionTest.txt\",\"w\",\"UTF-8\") as file:\n",
    "    for f1,f2,f3,f4,f5,l in zip(feature1,feature2,feature3,feature4,feature5,label):\n",
    "        file.write(str(f1)+\",\"+str(f2)+\",\"+str(f3)+\",\"+str(f4)+\",\"+str(f5)+\",\"+str(l)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff23323-d35c-458e-adaf-5400d998c11a",
   "metadata": {},
   "source": [
    "### Sigmoideal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbe854a-7dee-4ca3-b76f-3b14efdcf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import a mathematical library\n",
    "import math\n",
    "\"\"\"\n",
    "Define a Python function that calculates the sigmoid function\n",
    "Input: float\n",
    "Output: float (value between 0 and 1)\n",
    "\"\"\"\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60bf6e-7763-48ca-bfff-550830a431b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define a Python function that obtain the gradient\n",
    "Input: list - sampleList\n",
    "Input: list - weights\n",
    "Output: float (gradient)\n",
    "Note: For this function we use the sigmoid function for the first time.\n",
    "\"\"\"\n",
    "def gradient(sampleList, weights):\n",
    "    sumElements=0.0\n",
    "    for x,y in zip(sampleList,weights):\n",
    "        #Multiply weights and feature elements and add all values\n",
    "        sumElements += (x*y)\n",
    "        #Return the sigmoid of previous addition.\n",
    "        return sigmoid(sumElements)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define a Python function that given a training list, labels, feature number and iterations, calculates the␣\n",
    "→stochastic\n",
    "gradient ascent\n",
    "Input: list - trainingLists\n",
    "Input: list - trainingLabels\n",
    "Input: int - featureNumber\n",
    "Input: int - iterarions\n",
    "Output: list (optimal weights)\n",
    "\"\"\"\n",
    "def stochasticGradientAscent(trainingLists: List[List], trainingLabels: List, featureNumber, iterations=150, learning_rate=0.01):\n",
    "    #Get the number of training samples\n",
    "    sampleNumber=len(trainingLists)\n",
    "    #Create a list of N fatures (featureNumber) for saving optimal weights (1.0 as initial value)\n",
    "    weights=[1.0] * featureNumber\n",
    "    #Iterate a fixed number of times for getting optimal weights\n",
    "    for x in range(iterations):\n",
    "        #Get the index number of training samples\n",
    "        sampleIndex = list(range(sampleNumber))\n",
    "        #For each training sample do the following\n",
    "        for y in range(sampleNumber):\n",
    "\n",
    "        \"\"\"\n",
    "        Alpha is the learning rate and controls how much the coefficients (and therefore the model)\n",
    "        changes or learns each time it is updated.\n",
    "        Alpha decreases as the number of iterations increases, but it never reaches 0\n",
    "        \"\"\"\n",
    "        alpha=4/(1.0+x+y)+learning_rate # the operation before the learning rate is a way of optimizing the alpha\n",
    "        \n",
    "        #Randomly obtain an index of one of training samples\n",
    "        \"\"\"\n",
    "        Here, you’re randomly selecting each instance to use in updating the weights.\n",
    "        This will reduce the small periodic variations that can be present if we analyze\n",
    "        everything sequentially\n",
    "        \"\"\"\n",
    "        randIndex = int(random.uniform(0,len(sampleIndex)))\n",
    "        \n",
    "        #Obtain the gradient from the current training sample and weights\n",
    "        sampleGradient=gradient(trainingLists[randIndex],weights)\n",
    "        \n",
    "        #Check the error rate\n",
    "        error=trainingLabels[randIndex]-sampleGradient\n",
    "        \n",
    "        \"\"\"\n",
    "        we are calculating the error between the actual class and the predicted class and\n",
    "        then moving in the direction of that error (CURRENT TRAINING PROCESS)\n",
    "        \"\"\"\n",
    "        temp=[]\n",
    "        for index in range(featureNumber):\n",
    "            temp.append(alpha*(error*trainingLists[randIndex][index]))\n",
    "            for z in range(featureNumber):\n",
    "                weights[z]= weights[z] + temp[z]\n",
    "                del(sampleIndex[randIndex])\n",
    "\n",
    "    return weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
